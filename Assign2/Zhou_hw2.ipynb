{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jingyuan Zhou\n",
    "# Assignment 2: Text Classification\n",
    "# Implementation/Experimentation with Linear Text Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk.corpus",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-9b2acd570c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# from nltk.stem import WordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk.corpus"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from scipy.sparse import lil_matrix, coo_matrix, csr_matrix\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-b06499430ee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk"
     ]
    }
   ],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_table('sst3.train', header = None)\n",
    "train.columns = ['text', 'label']\n",
    "\n",
    "dev = pd.read_table('sst3.dev', header = None)\n",
    "dev.columns = ['text', 'label']\n",
    "\n",
    "devtest = pd.read_table('sst3.devtest', header = None)\n",
    "devtest.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(t):\n",
    "#     l = t.lower().translate(string.maketrans(\"\",\"\"), string.punctuation).strip().split()\n",
    "#     temp = [WordNetLemmatizer().lemmatize(x) for x in l if x != '' and x not in stopwords]\n",
    "    l = t.strip().split()\n",
    "    return l\n",
    "\n",
    "def process_df(df):\n",
    "    df['tokens'] = df['text'].apply(lambda x: process(x), 1)\n",
    "    df.reset_index(inplace = True)\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Building a Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_df(train)\n",
    "process_df(dev)\n",
    "process_df(devtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>2</td>\n",
       "      <td>[The, Rock, is, destined, to, be, the, 21st, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>is destined to be the 21st Century 's new `` C...</td>\n",
       "      <td>2</td>\n",
       "      <td>[is, destined, to, be, the, 21st, Century, 's,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>is destined to be the 21st Century 's new `` C...</td>\n",
       "      <td>2</td>\n",
       "      <td>[is, destined, to, be, the, 21st, Century, 's,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  label  \\\n",
       "0      0  The Rock is destined to be the 21st Century 's...      2   \n",
       "1      1  is destined to be the 21st Century 's new `` C...      2   \n",
       "2      2  is destined to be the 21st Century 's new `` C...      2   \n",
       "\n",
       "                                              tokens  \n",
       "0  [The, Rock, is, destined, to, be, the, 21st, C...  \n",
       "1  [is, destined, to, be, the, 21st, Century, 's,...  \n",
       "2  [is, destined, to, be, the, 21st, Century, 's,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(set(sum(list(train['tokens']), [])))\n",
    "\n",
    "# create feature functions based on training set\n",
    "feature_index_dic = {}\n",
    "ind = 0\n",
    "ind_0 = []\n",
    "ind_1 = []\n",
    "ind_2 = []\n",
    "for word in vocab:\n",
    "    for i in range(3):\n",
    "        feature_index_dic[(word, i)] = ind\n",
    "        if i == 0:\n",
    "            ind_0.append(ind)\n",
    "        elif i == 1:\n",
    "            ind_1.append(ind)\n",
    "        else:\n",
    "            ind_2.append(ind)\n",
    "        ind += 1\n",
    "\n",
    "label = [a[1] for a in feature_index_dic.keys()] #for train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18106"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def populate_feature(l, i_list, j_list, v_list, istrain = True):\n",
    "    i = l['index']\n",
    "    words = l['tokens']\n",
    "    if istrain:\n",
    "        label = l['label']\n",
    "        for w in words:\n",
    "            j = feature_index_dic[(w, label)]\n",
    "            i_list.append(i)\n",
    "            j_list.append(j)\n",
    "            v_list.append(1)\n",
    "    else:\n",
    "        for w in words:\n",
    "            for label in (0, 1, 2):\n",
    "                j = feature_index_dic[(w, label)]\n",
    "                i_list.append(i)\n",
    "                j_list.append(j)\n",
    "                v_list.append(1)               \n",
    "            \n",
    "#             l[(w, label)] = 1\n",
    "#             print 'Processed ('+str(w)+str(label)+')'\n",
    "\n",
    "def vectorize(df, istrain = True):\n",
    "    i_list = []\n",
    "    j_list = []\n",
    "    v_list = []\n",
    "    discard = df.apply(lambda x: populate_feature(x, i_list, j_list, v_list, istrain), 1)\n",
    "    f_M = coo_matrix((v_list, (i_list, j_list)), shape=(len(train), len(feature_index_dic)))\n",
    "    return f_M.tocsr()\n",
    "\n",
    "def classify(f_v, w):\n",
    "    v_0 = np.dot(f_v[ind_0], w[ind_0])\n",
    "    v_1 = np.dot(f_v[ind_1], w[ind_1])\n",
    "    v_2 = np.dot(f_v[ind_2], w[ind_2])\n",
    "#     print v\n",
    "    return np.argmax([v_0, v_1, v_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_M_train = vectorize(train)\n",
    "#f_M_test = vectorize(test, istrain = False)\n",
    "#f_M_dev = vectorize(dev, istrain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index (54315) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3850fca6b970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mf_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_M_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.toarray()[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mf_v\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d884f2b423ab>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(f_v, w)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mv_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mv_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mv_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# [[1,2],??]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# [[1,2],j] or [[1,2],1:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0missequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0;31m# [[1,2],[1,2]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36mextractor\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mmin_indx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmin_indx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/sparse/csr.pyc\u001b[0m in \u001b[0;36mcheck_bounds\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mmin_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index (54315) out of range"
     ]
    }
   ],
   "source": [
    "w = np.zeros(len(feature_index_dic))#.reshape(len(feature_index_dic),1) # for each epoch\n",
    "for i in range(len(train)):\n",
    "    f_v = f_M_train.getrow(i)\n",
    "    c = classify(f_v.toarray()[0], w)\n",
    "    y_2 = [int(l == c) for l in label]\n",
    "    w = w + 0.01*f_v - 0.01* np.array(y_2)\n",
    "    if i%20000 == 0:\n",
    "        print 'finished'+str(i)+'entries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(f_M):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# at first, get all the features\n",
    "def populate_feature(l, features):\n",
    "    i = l['index']\n",
    "    words = l['tokens']\n",
    "    label = l['label']\n",
    "    for w in words:\n",
    "        if (w, label) not in features:\n",
    "            features.append((w, label))\n",
    "               \n",
    "features = []\n",
    "discard = train.apply(lambda x: populate_feature(x, features), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36964"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score (word, y, f_weight_dic):\n",
    "    try:\n",
    "        return f_weight_dic[(word, y)]\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_p(tokens, f_weight_dic):\n",
    "    scores = []\n",
    "    for y in range(2):\n",
    "        s = 0\n",
    "        for word in tokens:\n",
    "            s += score(word, y, f_weight_dic)\n",
    "        scores.append(s)\n",
    "    return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_h(tokens, label, f_weight_dic):\n",
    "    scores = []\n",
    "    for y in range(2):\n",
    "        s = 0\n",
    "        for word in tokens:\n",
    "            s += score(word, y, f_weight_dic) + (y != label)   \n",
    "        scores.append(s)\n",
    "    return np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(df, f_weigt_dic):\n",
    "    acc = 0\n",
    "    for i in range(len(df)):\n",
    "        line = df.iloc[i,:]\n",
    "        label = line['label']\n",
    "        tokens = list(set(line['tokens']))\n",
    "        acc += (label==classify(tokens, f_weight_dic))\n",
    "#     print acc\n",
    "    r = acc*100/float(len(df))\n",
    "    print format(r, '.2f')+'%'\n",
    "    return r\n",
    "#     return format(acc*100/len(dev), '.2f')+'%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perceptron_loss(word, label, f_weight_dic):\n",
    "    for y in range(3):\n",
    "        if y == label:\n",
    "            f_weight_dic[(word, y)] = f_weight_dic[(word, y)] + 0.01-\\\n",
    "                            0.01*(label == classify_p(tokens, f_weight_dic))\n",
    "        else:\n",
    "            try:\n",
    "                f_weight_dic[(word, y)] = f_weight_dic[(word, y)]-\\\n",
    "                                    0.01*(label == classify_p(tokens, f_weight_dic))\n",
    "            except:\n",
    "                continue\n",
    "    return f_weight_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hinge_loss(word, label, f_weight_dic):\n",
    "    for y in range(3):\n",
    "        if y == label:\n",
    "            f_weight_dic[(word, y)] = f_weight_dic[(word, y)] + 0.01-\\\n",
    "                            0.01*(label == classify_h(tokens, label, f_weight_dic))\n",
    "        else:\n",
    "            try:\n",
    "                f_weight_dic[(word, y)] = f_weight_dic[(word, y)]-\\\n",
    "                                    0.01*(label == classify_(tokens, label, f_weight_dic))\n",
    "            except:\n",
    "                continue  \n",
    "    return f_weight_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ssd(loss, data = train):\n",
    "    best_score = 0\n",
    "    f_weight_dic = dict.fromkeys(features, 0)\n",
    "\n",
    "\n",
    "    for epoch in range(1, 21):\n",
    "        for i in range(len(train)):\n",
    "            line = train.iloc[i,:]\n",
    "            label = line['label']\n",
    "            tokens = set(line['tokens'])\n",
    "            for word in tokens:\n",
    "                if loss == 'perceptron':\n",
    "                    f_weight_dic = perceptron_loss(word, label, f_weight_dic)\n",
    "                \n",
    "                else:\n",
    "                    f_weight_dic = hinge_loss(word, label, f_weight_dic)\n",
    "    #         if i%20000 == 0 and i!= 0:\n",
    "    #             print 'Epoch %d position %d ' % (epoch, i)\n",
    "    #             res = test(dev, f_weight_dic)\n",
    "    #             if res > best_score:\n",
    "    #                 best_score = res\n",
    "    #                 best_w = f_weight_dic\n",
    "            break\n",
    "        break\n",
    "        print 'End of epoch '+ str(epoch)\n",
    "        res = test(dev, f_weight_dic)\n",
    "        break\n",
    "    #     if res > best_score:\n",
    "    #         best_score = res\n",
    "    #         best_w = f_weight_dic\n",
    "#     test(devtest, f_weight_dic\n",
    "    return f_weight_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_weight_dic = ssd('perceptron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1\n",
      "39.82%\n",
      "37.93%\n"
     ]
    }
   ],
   "source": [
    "ssd('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "line = dev.iloc[0,:]\n",
    "label = line['label']\n",
    "tokens = line['tokens']\n",
    "\n",
    "classify(tokens, f_weight_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = train.iloc[0, :]['tokens']\n",
    "label = train.iloc[0, :]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 2)\n",
      "0.01\n",
      "('Rock', 2)\n",
      "0.01\n",
      "('is', 2)\n",
      "0.01\n",
      "('destined', 2)\n",
      "0.01\n",
      "('to', 2)\n",
      "0.01\n",
      "('be', 2)\n",
      "0.01\n",
      "('the', 2)\n",
      "0.01\n",
      "('21st', 2)\n",
      "0.01\n",
      "('Century', 2)\n",
      "0.01\n",
      "(\"'s\", 2)\n",
      "0.01\n",
      "('new', 2)\n",
      "0.01\n",
      "('``', 2)\n",
      "0.01\n",
      "('Conan', 2)\n",
      "0.01\n",
      "(\"''\", 2)\n",
      "0.01\n",
      "('and', 2)\n",
      "0.01\n",
      "('that', 2)\n",
      "0.01\n",
      "('he', 2)\n",
      "0.01\n",
      "(\"'s\", 2)\n",
      "0.01\n",
      "('going', 2)\n",
      "0.01\n",
      "('to', 2)\n",
      "0.01\n",
      "('make', 2)\n",
      "0.01\n",
      "('a', 2)\n",
      "0.01\n",
      "('splash', 2)\n",
      "0.01\n",
      "('even', 2)\n",
      "0.01\n",
      "('greater', 2)\n",
      "0.01\n",
      "('than', 2)\n",
      "0.01\n",
      "('Arnold', 2)\n",
      "0.01\n",
      "('Schwarzenegger', 2)\n",
      "0.01\n",
      "(',', 2)\n",
      "0.01\n",
      "('Jean-Claud', 2)\n",
      "0.01\n",
      "('Van', 2)\n",
      "0.01\n",
      "('Damme', 2)\n",
      "0.01\n",
      "('or', 2)\n",
      "0.01\n",
      "('Steven', 2)\n",
      "0.01\n",
      "('Segal', 2)\n",
      "0.01\n",
      "('.', 2)\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "t = zip(tokens, [label]*len(tokens))\n",
    "for i in t:\n",
    "    print i\n",
    "    print f_weight_dic[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Feature Weight Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'words': [a[0] for a in f_weight_dic.keys()], 'label': [a[1] for a in f_weight_dic.keys()],\\\n",
    "     'weight': f_weight_dic.values()}\n",
    "f_w_df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10624</th>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>punches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>lacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23338</th>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>amoral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28358</th>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>matter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>wartime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14333</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>slavery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5594</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>institutionalized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24424</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>spook-a-rama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10175</th>\n",
       "      <td>0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>befallen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  weight              words\n",
       "10624      0    0.03            punches\n",
       "13143      0    0.03              lacks\n",
       "23338      0    0.03             amoral\n",
       "28358      0    0.03             matter\n",
       "3730       0    0.03                  a\n",
       "4342       0    0.03            wartime\n",
       "14333      0    0.02            slavery\n",
       "5594       0    0.02  institutionalized\n",
       "24424      0    0.02       spook-a-rama\n",
       "10175      0    0.02           befallen"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_w_df[f_w_df['label'] == 0].sort_values(by = 'weight', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34863</th>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>hopeful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29667</th>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>band</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>1</td>\n",
       "      <td>0.12</td>\n",
       "      <td>son</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11356</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9552</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>producer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30525</th>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>recognizable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22385</th>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31152</th>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16067</th>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17446</th>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>punches</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  weight         words\n",
       "34863      1    0.16       hopeful\n",
       "29667      1    0.12          band\n",
       "2572       1    0.12           son\n",
       "11356      1    0.10         China\n",
       "9552       1    0.10      producer\n",
       "30525      1    0.10  recognizable\n",
       "22385      1    0.09        relief\n",
       "31152      1    0.09          eyes\n",
       "16067      1    0.09          post\n",
       "17446      1    0.09       punches"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_w_df[f_w_df['label'] == 1].sort_values(by = 'weight', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2313</th>\n",
       "      <td>2</td>\n",
       "      <td>112.94</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7156</th>\n",
       "      <td>2</td>\n",
       "      <td>92.48</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22210</th>\n",
       "      <td>2</td>\n",
       "      <td>81.65</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22946</th>\n",
       "      <td>2</td>\n",
       "      <td>79.84</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30671</th>\n",
       "      <td>2</td>\n",
       "      <td>71.43</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18262</th>\n",
       "      <td>2</td>\n",
       "      <td>44.56</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5552</th>\n",
       "      <td>2</td>\n",
       "      <td>33.39</td>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>2</td>\n",
       "      <td>32.69</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18670</th>\n",
       "      <td>2</td>\n",
       "      <td>28.00</td>\n",
       "      <td>film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15597</th>\n",
       "      <td>2</td>\n",
       "      <td>26.59</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  weight words\n",
       "2313       2  112.94   and\n",
       "7156       2   92.48     ,\n",
       "22210      2   81.65   the\n",
       "22946      2   79.84     a\n",
       "30671      2   71.43    of\n",
       "18262      2   44.56     .\n",
       "5552       2   33.39    's\n",
       "2661       2   32.69  with\n",
       "18670      2   28.00  film\n",
       "15597      2   26.59  that"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_w_df[f_w_df['label'] == 2].sort_values(by = 'weight', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Error Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Feature Engineering\n",
    "\n",
    "### 1.5.1 New feature template 1: lowercase words, remove punctuation and lemmatize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_table('sst3.train', header = None)\n",
    "train.columns = ['text', 'label']\n",
    "\n",
    "dev = pd.read_table('sst3.dev', header = None)\n",
    "dev.columns = ['text', 'label']\n",
    "\n",
    "devtest = pd.read_table('sst3.devtest', header = None)\n",
    "devtest.columns = ['text', 'label'] \n",
    "\n",
    "\n",
    "def process_1(t):\n",
    "    l = t.lower().translate(string.maketrans(\"\",\"\"), string.punctuation).strip().split()\n",
    "    temp = [WordNetLemmatizer().lemmatize(x) for x in l if x != '' and x not in stopwords]\n",
    "\n",
    "    return temp\n",
    "\n",
    "def process_df_1(df):\n",
    "    df['tokens'] = df['text'].apply(lambda x: process_1(x), 1)\n",
    "    df.reset_index(inplace = True)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-d40dc9c0c297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_df_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprocess_df_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprocess_df_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-194-3e7efbaac3b2>\u001b[0m in \u001b[0;36mprocess_df_1\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_df_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprocess_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     return df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2167\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2169\u001b[0;31m         \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:62578)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-194-3e7efbaac3b2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_df_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprocess_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     return df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-194-3e7efbaac3b2>\u001b[0m in \u001b[0;36mprocess_1\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "process_df_1(train)\n",
    "process_df_1(dev)\n",
    "process_df_1(devtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
